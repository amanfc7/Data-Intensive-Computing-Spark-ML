{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452b5d7-9133-4106-9780-2b348b3914bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1) RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b59a30-dd88-4295-9ec5-3b37a640f3ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "# for initializing the Spark context and spark session:\n",
    "\n",
    "sc = SparkContext(appName=\"ChiSquareRDD\")\n",
    "spark = SparkSession.builder.appName(\"ChiSquareRDD\").getOrCreate()\n",
    "\n",
    "# below is function for loading stopwords from stopwords.txt:\n",
    "\n",
    "def loading_stopwords(stopwords_file):\n",
    "    with open(stopwords_file, 'r') as f:\n",
    "        stopwords = {line.strip() for line in f}\n",
    "    return stopwords\n",
    "\n",
    "# now, the function for loading and preprocessing steps of the dataset:\n",
    "\n",
    "def preprocessing(line, stopwords):\n",
    "    review = json.loads(line)\n",
    "    reviewText = review['reviewText']\n",
    "    category = review['category']\n",
    "    \n",
    "    words = re.split(r'[^\\w]+', reviewText.lower())\n",
    "    words = [word for word in words if len(word) > 1 and word.isalpha() and word not in stopwords]   # to filter words only containing characters\n",
    "    return (category, words)\n",
    "\n",
    "# it will load stopwords from stopwords.txt:\n",
    "\n",
    "stopwords_file = \"src/stopwords.txt\"\n",
    "stopwords = loading_stopwords(stopwords_file)\n",
    "\n",
    "\n",
    "data = sc.textFile(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")     # to use the reduced dataset for development\n",
    "rdd = data.map(lambda line: preprocessing(line, stopwords)).cache()\n",
    "\n",
    "# function for calculation of the chi-square values:\n",
    "\n",
    "def chi_square(category_word_counts, total_counts, category_counts, total_docs):\n",
    "    chi_square_value = defaultdict(float)\n",
    "    for category, words in category_word_counts.items():\n",
    "        for word, count in words.items():\n",
    "            A = count\n",
    "            B = category_counts[category] - count\n",
    "            C = total_counts[word] - A\n",
    "            N = total_docs\n",
    "            D = N - (A + B + C)\n",
    "            numerator = N * (A * D - B * C) ** 2\n",
    "            denominator = (A + B) * (C + D) * (A + C) * (B + D)\n",
    "            if denominator != 0:\n",
    "                chi_square_value[(category, word)] = numerator / denominator\n",
    "    return chi_square_value\n",
    "\n",
    "# collecting category_word_count, total_count, category_count, total_docs using RDD's:\n",
    "\n",
    "category_word_count = rdd.flatMapValues(lambda words: words) \\\n",
    "                          .map(lambda x: ((x[0], x[1]), 1)) \\\n",
    "                          .reduceByKey(lambda x, y: x + y) \\\n",
    "                          .map(lambda x: (x[0][0], {x[0][1]: x[1]})) \\\n",
    "                          .reduceByKey(lambda x, y: {**x, **y}) \\\n",
    "                          .collectAsMap()\n",
    "\n",
    "total_count = rdd.flatMap(lambda x: set(x[1])) \\\n",
    "                  .map(lambda word: (word, 1)) \\\n",
    "                  .reduceByKey(lambda x, y: x + y) \\\n",
    "                  .collectAsMap()\n",
    "\n",
    "category_count = rdd.map(lambda x: (x[0], 1)) \\\n",
    "                     .reduceByKey(lambda x, y: x + y) \\\n",
    "                     .collectAsMap()\n",
    "\n",
    "total_docs = rdd.count()\n",
    "\n",
    "# computing the chi-square value:\n",
    "\n",
    "chi_square_value = chi_square(category_word_count, total_count, category_count, total_docs)\n",
    "\n",
    "# now, to obtain top 75 terms for each category based on the chi-square values:\n",
    "\n",
    "top_terms_per_category = {}\n",
    "for category in category_word_count:\n",
    "    sorted_terms = sorted([(word, chi_square_value[(category, word)]) for word in category_word_count[category]],\n",
    "                          key=lambda x: -x[1])\n",
    "    top_terms_per_category[category] = sorted_terms[:75]\n",
    "\n",
    "# to obtain top terms from all categories:\n",
    "\n",
    "top_terms = set()\n",
    "for terms in top_terms_per_category.values():\n",
    "    top_terms.update(term[0] for term in terms)\n",
    "\n",
    "# creating a joined dictionary and for the output:\n",
    "\n",
    "joined_dictionary = sorted(top_terms)\n",
    "\n",
    "output_content = []    # to prepare the output \n",
    "\n",
    "\n",
    "# for writing the top terms per category:\n",
    "\n",
    "for category in sorted(top_terms_per_category.keys()):\n",
    "    terms = top_terms_per_category[category]\n",
    "    output_content.append(f\"Category: {category}\")\n",
    "    for term in terms:\n",
    "        output_content.append(f\"{term[0]}: {term[1]:.4f}\")   \n",
    "    output_content.append(\"\\n\")\n",
    "    \n",
    "\n",
    "# for seaparating and leaving some space between the chi square values of terms and the dictionary:\n",
    "\n",
    "output_content.append(\"=\" * 100)\n",
    "output_content.append(\"DICTIONARY:\")\n",
    "output_content.append(\"=\" * 100)\n",
    "\n",
    "# to add the joined dictionary in the output file:\n",
    "\n",
    "output_content.append(\" \".join(joined_dictionary))\n",
    "\n",
    "# for creating the output file with all the results:\n",
    "\n",
    "output_file = \"src/output_rdd.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for line in output_content:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "sc.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b7c6d3-ae8c-435e-af67-f5fd9ffdbbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2) Datasets/DataFrames: Spark ML and Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002707ab-c5c3-4186-90c4-7a8bbe092ed6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, lower\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector, StringIndexer\n",
    "\n",
    "# to initialize Spark context:\n",
    "\n",
    "sc = SparkContext(appName=\"ChiSquareDF\")\n",
    "spark = SparkSession.builder.appName(\"ChiSquareDF\").getOrCreate()\n",
    "\n",
    "# for loading the stopwords:\n",
    "\n",
    "stopwords = set()\n",
    "with open(\"src/stopwords.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip())\n",
    "\n",
    "# now, function to load and preprocess the dataset:\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = spark.read.json(file_path)\n",
    "    # Normalize text\n",
    "    data = data.withColumn(\"text\", lower(col(\"reviewText\")))\n",
    "    data = data.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "    return data.select(col(\"category\"), col(\"text\"))\n",
    "\n",
    "# loading data from the amazon review dataset for development:\n",
    "\n",
    "data = load_and_preprocess_data(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")\n",
    "\n",
    "# to define the structure and different stages of the pipeline:\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=list(stopwords))\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\", vocabSize=2000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"label\")\n",
    "\n",
    "# Creating of a pipeline:\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, vectorizer, idf, selector])\n",
    "\n",
    "# for fitting the pipeline to the given data:\n",
    "\n",
    "model = pipeline.fit(data)\n",
    "\n",
    "# for transforming the data:\n",
    "\n",
    "result = model.transform(data)\n",
    "\n",
    "# to extract the selected features (terms/words) from the data:\n",
    "\n",
    "selected_indices = model.stages[-1].selectedFeatures\n",
    "vocab = model.stages[3].vocabulary\n",
    "selected_terms = [vocab[i] for i in selected_indices]\n",
    "\n",
    "# Collection of the TF-IDF vectors for the first 2000 terms:\n",
    "\n",
    "tfidf_vectors_rdd = result.select(\"selected_features\").rdd.map(lambda row: row.selected_features.toArray())\n",
    "tfidf_vectors = tfidf_vectors_rdd.take(2000)  # Limiting to 2000 terms for memory efficiency\n",
    "\n",
    "# writing and finally saving the selected terms and their corresponding TF-IDF vectors to a file:\n",
    "\n",
    "output_file = \"src/output_ds.txt\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for term, vec in zip(selected_terms, tfidf_vectors):\n",
    "        f.write(f\"{term}: {vec}\\n\")\n",
    "\n",
    "\n",
    "sc.stop()    # to stop the Spark context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982c6f6-d3e2-414c-bd36-e7639be73bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3) Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f58ac2-2673-4d3a-b69a-f0eecafa0cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, regexp_replace\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF, Normalizer, StringIndexer, ChiSqSelector\n",
    "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "\n",
    "# initializing Spark context:\n",
    "\n",
    "sc = SparkContext(appName=\"TextClassification\")\n",
    "spark = SparkSession.builder.appName(\"TextClassification\").getOrCreate()\n",
    "\n",
    "# Loading the stopwords:\n",
    "\n",
    "stopwords = set()\n",
    "with open(\"src/stopwords.txt\", \"r\") as f:\n",
    "    for line in f:\n",
    "        stopwords.add(line.strip())\n",
    "\n",
    "# Loading and preprocessing the dataset:\n",
    "\n",
    "def load_and_preprocess_data(file_path):\n",
    "    data = spark.read.json(file_path)\n",
    "    data = data.withColumn(\"text\", lower(col(\"reviewText\")))\n",
    "    data = data.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z\\\\s]\", \"\"))\n",
    "    return data.select(col(\"category\"), col(\"text\"))\n",
    "\n",
    "# to load data from reviews dataset:\n",
    "\n",
    "data = load_and_preprocess_data(\"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\")\n",
    "\n",
    "# to define stages of the pipeline:\n",
    "\n",
    "indexer = StringIndexer(inputCol=\"category\", outputCol=\"label\")\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "stopwords_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\", stopWords=list(stopwords))\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=2000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "selector = ChiSqSelector(numTopFeatures=2000, featuresCol=\"features\", outputCol=\"selected_features\", labelCol=\"label\")\n",
    "normalizer = Normalizer(inputCol=\"selected_features\", outputCol=\"normalized_features\", p=2.0)\n",
    "svm = LinearSVC(featuresCol=\"normalized_features\", labelCol=\"label\")\n",
    "ovr_classifier = OneVsRest(classifier=svm)\n",
    "\n",
    "# to create a pipeline wityh various stages:\n",
    "\n",
    "pipeline = Pipeline(stages=[indexer, tokenizer, stopwords_remover, hashingTF, idf, selector, normalizer, ovr_classifier])\n",
    "\n",
    "# for splitting the data into training, validation, and test sets:\n",
    "\n",
    "(training_data, validation_data, test_data) = data.randomSplit([0.7, 0.2, 0.1], seed=12345)\n",
    "\n",
    "# to define parameter grid for grid search (regularization parameter (3 values), standardization of training features (2 values), and maximum number of iterations (2 values)):\n",
    "\n",
    "param_grid = ParamGridBuilder() \\\n",
    "    .addGrid(hashingTF.numFeatures, [500, 2000]) \\\n",
    "    .addGrid(svm.regParam, [0.01, 0.1, 1.0]) \\\n",
    "    .addGrid(svm.maxIter, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "# evaluator for Multiclass Classification (F1 score):\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "# to perform grid search manually:\n",
    "\n",
    "for params in param_grid:\n",
    "    model = pipeline.copy(params).fit(training_data)\n",
    "    predictions = model.transform(validation_data)\n",
    "    f1_score = evaluator.evaluate(predictions)\n",
    "    print(f\"Parameters: {params}, F1 Score on Validation Set: {f1_score}\")\n",
    "\n",
    "# to make predictions on test data:\n",
    "\n",
    "    predictions_test = model.transform(test_data)\n",
    "    f1_score_test = evaluator.evaluate(predictions_test)\n",
    "    print(f\"F1 Score on Test Set with Parameters {params}: {f1_score_test}\")\n",
    "\n",
    "sc.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (DIC24)",
   "language": "python",
   "name": "python3_dic24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
